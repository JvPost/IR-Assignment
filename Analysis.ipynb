{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader \n",
    "from pyserini.search import SimpleSearcher\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "config = json.loads(open(\"config.json\", \"r\").read())\n",
    "index_path = config[\"index_path\"]\n",
    "topics_path = config[\"topics_path\"]\n",
    "qrels_path = config[\"qrels_path\"]\n",
    "index_path\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_50 = api.load('glove-wiki-gigaword-50')\n",
    "# wiki_300 = api.load('glove-wiki-gigaword-300')\n",
    "# wiki_fast = api.load('fasttext-wiki-news-subwords-300')\n",
    "# google = api.load('word2vec-google-news-300')\n",
    "\n",
    "wiki_300 = KeyedVectors.load_word2vec_format('~/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('organised', 0.6641563773155212),\n",
       " ('criminal', 0.5866537094116211),\n",
       " ('crimes', 0.5811451077461243),\n",
       " ('organizations', 0.5762122273445129),\n",
       " ('organization', 0.5691279172897339),\n",
       " ('trafficking', 0.5649696588516235),\n",
       " ('terrorism', 0.5468315482139587),\n",
       " ('activities', 0.5334997773170471),\n",
       " ('involved', 0.5092121958732605),\n",
       " ('corruption', 0.5076022744178772)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_300.most_similar(['international', 'organized', 'crime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT933-6678 27.772499084472656\n",
      "FT934-5418 27.051700592041016\n",
      "LA040190-0178 25.658899307250977\n",
      "LA041090-0148 25.251100540161133\n",
      "FBIS3-42547 25.09709930419922\n",
      "FBIS4-46650 24.89459991455078\n",
      "LA050390-0109 24.694799423217773\n",
      "LA081090-0078 24.25629997253418\n",
      "LA071490-0091 24.17329978942871\n",
      "LA060890-0124 24.135099411010742\n",
      "LA081790-0164 24.1112003326416\n",
      "LA052890-0021 23.854400634765625\n",
      "LA070390-0084 23.843700408935547\n",
      "LA051590-0074 23.284799575805664\n",
      "LA051490-0110 23.137399673461914\n",
      "LA080990-0242 23.053499221801758\n",
      "FT921-7107 22.52090072631836\n",
      "FT933-6946 22.48889923095703\n",
      "LA071090-0047 22.33839988708496\n",
      "FT944-128 22.332000732421875\n"
     ]
    }
   ],
   "source": [
    "topic = 'hubble space telescope'\n",
    "et = expand_query(topic, wiki_300, len(topic.split()))\n",
    "\n",
    "hits = Searcher.search(et, 20)\n",
    "for h in hits:\n",
    "    print(h.docid, h.score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hubble space telescope nasa spacecraft observatory'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_topic = expand_query(topic, wiki_300, len(topic.split()), 0)\n",
    "expanded_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:13<00:00, 18.61it/s]\n",
      "100%|██████████| 250/250 [00:52<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics(topics_path)\n",
    "#k = nro. results per topic\n",
    "#n = nro. extra words\n",
    "def make_results(model, n:int = 0, dynamic = False, k:int = 25, threshold = .7):\n",
    "    results = \"\"\n",
    "    for i in tqdm(topics):\n",
    "        ranking = \"\"\n",
    "        if dynamic:\n",
    "            extra_words_count = 0;\n",
    "            for word in topics[i].split():\n",
    "                if word not in stopwords.words('english'):\n",
    "                    extra_words_count+=1\n",
    "            expanded_topic = expand_query(topics[i], model, extra_words_count, threshold)\n",
    "            hits = Searcher.search(expanded_topic, k=k)\n",
    "        elif n > 0 and not dynamic: \n",
    "            expanded_topic = expand_query(topics[i], model, n, threshold)\n",
    "            hits = Searcher.search(expanded_topic, k=k)\n",
    "        else:\n",
    "            hits = Searcher.search(topics[i], k=k)\n",
    "            \n",
    "        for r, h in enumerate(hits):\n",
    "            ranking += f\"{i} 0 {h.docid} {r+1} {h.score} RUN1\\n\"\n",
    "        results += ranking\n",
    "        \n",
    "    filename = \"\"\n",
    "    filename = f'results/results_{n}_{k}_{threshold*100}.txt'\n",
    "            \n",
    "    f = open(filename, 'w')\n",
    "    f.write(results)\n",
    "    f.close()\n",
    "\n",
    "make_results(wiki_300, 0, k=1000 ) # no expanded query\n",
    "make_results(wiki_300, 'x', dynamic=True, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Poliomyelitis', 'and', 'Post-Polio', 'polio', 'measles', 'diphtheria']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Poliomyelitis and Post-Polio polio measles diphtheria'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 0.6708575925458083\n"
     ]
    }
   ],
   "source": [
    "metric_0 = 0\n",
    "k=1000\n",
    "labels_gen = query_labels_from_file(qrels_path, f'results/results_0_{k}_0.txt')\n",
    "r = 0\n",
    "for labels in labels_gen:\n",
    "    metric_0+=NDCG(labels, k)\n",
    "    r+=1\n",
    "    \n",
    "print(f\"0 {k} {metric_0/r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 1000 0.5824161580572926\n"
     ]
    }
   ],
   "source": [
    "metric = 0\n",
    "\n",
    "labels_gen = query_labels_from_file(qrels_path, f'results/results_x_{k}_0.txt')\n",
    "r = 0\n",
    "for labels in labels_gen:\n",
    "    metric+=NDCG(labels, k)\n",
    "    r+=1\n",
    "     \n",
    "print(f\"x {k} {metric/r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 492.12it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 525.74it/s]\n",
      "100%|██████████| 250/250 [00:00<00:00, 570.28it/s]\n",
      "  5%|▍         | 12/250 [00:00<00:06, 34.39it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'levitation-maglev' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4407/2578180429.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtaus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# run_query(k, n, tau)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mmake_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4407/757374752.py\u001b[0m in \u001b[0;36mmake_results\u001b[0;34m(model, n, dynamic, k, threshold)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mexpanded_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/IR-Assignment/utils.py\u001b[0m in \u001b[0;36mexpand_query\u001b[0;34m(topic, model, n, threshold)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mitr\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0msimilar_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar_words_from_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/IR-Assignment/utils.py\u001b[0m in \u001b[0;36mget_similar_words_from_sentence\u001b[0;34m(sentence, model, n, threshold)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_similar_words_from_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mextra_words_and_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mextra_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_words_and_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IR/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IR/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/IR/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'levitation-maglev' not present\""
     ]
    }
   ],
   "source": [
    "ks = [10, 25, 50] + list(range(100, 1100, 100))\n",
    "ns = list(range(0, 11))\n",
    "taus = [0.7, 0.8, 0.9]\n",
    "\n",
    "print(ks)\n",
    "\n",
    "for k in ks:\n",
    "    for n in ns:\n",
    "        for tau in taus:\n",
    "            # run_query(k, n, tau)\n",
    "            make_results(wiki_300, n,False,k,tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hubble space telescope nasa mission launch'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_doc_vectors(docs):\n",
    "    tfs = dict()\n",
    "    for d in docs:\n",
    "        tf = IndexReader.reader.get_document_vector(d)\n",
    "        for k in tf:\n",
    "            if k in tfs:\n",
    "                tfs[k] += tf[k]\n",
    "            else:\n",
    "                tfs[k] = tf[k]\n",
    "    return tfs\n",
    "\n",
    "def rank_words(doc_vectors, words):\n",
    "    top_words = {}\n",
    "    for w in words:\n",
    "        if w in doc_vectors:\n",
    "            if w in top_words:\n",
    "                top_words[w] += doc_vectors[w]\n",
    "            else:\n",
    "                top_words[w] = doc_vectors[w]\n",
    "    ordered_dict = dict(sorted(top_words.items(), key=lambda item: item[1], reverse=True))\n",
    "    return list(ordered_dict.keys())\n",
    "\n",
    "    \n",
    "def expand_query_using_relevance_feedback(model, topic, n = None, top_docs = 10):\n",
    "    if n is None:\n",
    "        n = 0\n",
    "        for word in topic.split():\n",
    "            if word not in stopwords.words('english'):\n",
    "                n += 1\n",
    "        \n",
    "    hits = Searcher.search(topic, top_docs)\n",
    "    doc_ids = [h.docid for h in hits]\n",
    "    doc_vector = merge_doc_vectors(doc_ids)\n",
    "    top_words = []\n",
    "    itr = 1\n",
    "    while len(top_words) <= n:\n",
    "        prev_len = len(top_words)\n",
    "        new_words = expand_query(topic, model, 10*n*itr).split()[n:]\n",
    "        potentials = rank_words(doc_vector, new_words)\n",
    "        top_words += potentials[(itr-1)*n:itr*n]\n",
    "        itr+=1\n",
    "        if prev_len == len(top_words) or itr >= 10:\n",
    "            break\n",
    "        \n",
    "    return topic + ' ' + ' '.join(top_words[:n])\n",
    "\n",
    "expand_query_using_relevance_feedback(wiki_300, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [04:14<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "#k = nro. results per topic\n",
    "#n = nro. extra words\n",
    "def make_results(model, n:int = 0, dynamic = False, k:int = 25, threshold = .7):\n",
    "    topics = get_topics(topics_path)\n",
    "    results = \"\"\n",
    "    for i in tqdm(topics):\n",
    "        ranking = \"\"\n",
    "        if dynamic:\n",
    "            extra_words_count = 0;\n",
    "            for word in topics[i].split():\n",
    "                if word not in stopwords.words('english'):\n",
    "                    extra_words_count+=1\n",
    "            expanded_topic = expand_query(topics[i], model, extra_words_count, threshold)\n",
    "            hits = Searcher.search(expanded_topic, k=k)\n",
    "        elif n > 0 and not dynamic: \n",
    "            expanded_topic = expand_query(topics[i], model, n, threshold)\n",
    "            hits = Searcher.search(expanded_topic, k=k)\n",
    "        else:\n",
    "            hits = Searcher.search(topics[i], k=k)\n",
    "            \n",
    "        for r, h in enumerate(hits):\n",
    "            ranking += f\"{i} 0 {h.docid} {r+1} {h.score} RUN1\\n\"\n",
    "        results += ranking\n",
    "        \n",
    "    filename = \"\"\n",
    "    filename = f'results/results_{n}_{k}_{threshold*100}.txt'\n",
    "            \n",
    "    f = open(filename, 'w')\n",
    "    f.write(results)\n",
    "    f.close()\n",
    "\n",
    "# make_results(wiki_300, 0, k=1000 ) # no expanded query\n",
    "# make_results(wiki_300, 'x', dynamic=True, k=1000)\n",
    "def make_relevancefeedback_results(model, k=25, top_docs=10):\n",
    "    results = \"\"\n",
    "    topics = get_topics(topics_path)\n",
    "    for i in tqdm(topics):\n",
    "        ranking = \"\"\n",
    "        expanded_topic = expand_query_using_relevance_feedback(model, topics[i], top_docs=top_docs)\n",
    "        hits = Searcher.search(expanded_topic, k=k)\n",
    "        \n",
    "        for r, h in enumerate(hits):\n",
    "            ranking += f\"{i} 0 {h.docid} {r+1} {h.score} RUN1\\n\"\n",
    "        results += ranking\n",
    "        \n",
    "    filename = f\"relevance_feedback_results/results_{top_docs}.txt\"\n",
    "    f = open(filename, 'w')\n",
    "    f.write(results)\n",
    "    f.close()\n",
    "        \n",
    "make_relevancefeedback_results(wiki_300, k=1000, top_docs=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 0.6708575925458083\n"
     ]
    }
   ],
   "source": [
    "metric_0 = 0\n",
    "k=1000\n",
    "labels_gen = query_labels_from_file(qrels_path, f'results/results_0_{k}_0.txt')\n",
    "r = 0\n",
    "for labels in labels_gen:\n",
    "    metric_0+=NDCG(labels, k)\n",
    "    r+=1\n",
    "    \n",
    "print(f\"0 {k} {metric_0/r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 1000 0.5824161580572926\n"
     ]
    }
   ],
   "source": [
    "metric = 0\n",
    "\n",
    "labels_gen = query_labels_from_file(qrels_path, f'results/results_x_{k}_0.txt')\n",
    "r = 0\n",
    "for labels in labels_gen:\n",
    "    metric+=NDCG(labels, k)\n",
    "    r+=1\n",
    "     \n",
    "print(f\"x {k} {metric/r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance_feedback 1000 0.5983220912453281\n"
     ]
    }
   ],
   "source": [
    "metric_relevance_feedback = 0\n",
    "p = 25\n",
    "labels_gen = query_labels_from_file(qrels_path, f\"relevance_feedback_results/results_{p}.txt\")\n",
    "r = 0\n",
    "for labels in labels_gen:\n",
    "    metric_relevance_feedback+= NDCG(labels, k)\n",
    "    r+=1\n",
    "    \n",
    "print(f\"relevance_feedback {k} {metric_relevance_feedback/r}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
